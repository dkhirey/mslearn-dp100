Azure Introduction -

1. Azure resources created alongside a workspace include:
	A storage account - used to store files used by the workspace as well as data for experiments and model training.
	An Application Insights instance, used to monitor predictive services in the workspace.
	An Azure Key Vault instance, used to manage secrets such as authentication keys and credentials used by the workspace.
	A container registry, created as-needed to manage containers for deployed models.
2. By default, the from_config method looks for a file named config.json in the folder containing the Python code file.
3. The Azure command-line interface (CLI) is a cross-platform command-line tool for managing Azure resources.  (azure-cli-ml)
4. Loggng with Run context -
	log: Record a single named value.
	log_list: Record a named list of values.
	log_row: Record a row with multiple columns.
	log_table: Record a dictionary as a table.
	log_image: Record an image file or a plot.
5. You can run an experiment inline using the start_logging method of the Experiment object.


Datasets -

1. Types of Datastores -
	Azure Storage (blob and file containers)
	Azure Data Lake stores
	Azure SQL Database
	Azure Databricks file system (DBFS)
2. Every workspace has two built-in datastores - 
	Azure Storage blob container, 
	Azure Storage file container
3. The workspace always includes a default datastore initially, this is the built-in workspaceblobstore datastore.
4. Types of Datasets -
	Tabular: The data is read from the dataset as a table. You should use this type of dataset when your data is consistently 	structured and you want to work with it in common tabular data structures, such as Pandas dataframes.
	File: The dataset presents a list of file paths that can be read as though from the file system. Use this type of dataset when your data is unstructured, or when you need to process the data at the file level
5. you use the as_named_input method of the dataset to specify a name for the dataset. Then in the script, you can retrieve the dataset by name from the run context's input_datasets collection without needing to retrieve it from the workspace.
6. you must specify a mode for the file dataset argument, which can be as_download or as_mount. 

Computes -

1. Types of Computes -
	Local compute 
	Compute clusters
	Attached compute 
	inference clusters - can only be used to deploy trained models as inferencing services
2. A managed compute target is one that is managed by Azure Machine Learning, such as an Azure Machine Learning compute cluster.
3. An unmanaged compute target is one that is defined and managed outside of the Azure Machine Learning workspace; for example, an Azure virtual machine or an Azure Databricks cluster.

Pipelines -

1. In Azure Machine Learning, a pipeline is a workflow of machine learning tasks in which each task is implemented as a step.
2. Types of steps -
	PythonScriptStep: Runs a specified Python script.
	DataTransferStep: Uses Azure Data Factory to copy data between data stores.
	DatabricksStep: Runs a notebook, script, or compiled JAR on a databricks cluster.
	AdlaStep: Runs a U-SQL job in Azure Data Lake Analytics.
	ParallelRunStep - Runs a Python script as a distributed task on multiple compute nodes.
3. The PipelineData object is a special kind of DataReference that:
	References a location in a datastore.
	Creates a data dependency between pipeline steps.
4. you can force all of them to run regardless of individual reuse configuration by setting the regenerate_outputs parameter
5. To control reuse for an individual step, you can set the allow_reuse parameter in the step configuration
6. To define parameters for a pipeline, create a PipelineParameter object for each parameter, and specify each parameter in at least one step.
7. You must define parameters for a pipeline before publishing it.
8. To schedule a pipeline to run at periodic intervals, you must define a ScheduleRecurrence that determines the run frequency, and use it to create a Schedule.
9. To schedule a pipeline to run whenever data changes, you must create a Schedule that monitors a specified path on a datastore.


Model as Realtime service -

1. You can deploy a model as a real-time web service to several kinds of compute target, including local compute, an Azure Machine Learning compute instance, an Azure Container Instance (ACI), an Azure Kubernetes Service (AKS) cluster, an Azure Function, or an Internet of Things (IoT) module.
2. Create the entry script (sometimes referred to as the scoring script)
	init(): Called when the service is initialized.
	run(raw_data): Called when new data is submitted to the service.
3. two kinds of authentication you can use:
	Key: Requests are authenticated by specifying the key associated with the service.
	Token: Requests are authenticated by providing a JSON Web Token (JWT).
4. Azure Container Instances (ACI) are suitable only for small models less than 1 GB in size.
Use it for low-scale CPU-based workloads that require less than 48 GB of RAM.
5. Microsoft recommends using single-node Azure Kubernetes Service (AKS) clusters for dev-test of larger models.
6. Using GPU for inference when deployed as a web service is supported only on AKS.
7. In a local deployment you can't use a reference to a model or environment registered in your Azure Machine Learning workspace.
8. az ml endpoint update command - You can only modify one aspect (traffic, scale settings, code, model, or environment) in a single update command. 
9. service.get-logs() command will only provide the last few-hundred lines of logs from an automatically selected instance.
10. When deploying to Azure Kubernetes Service, key-based authentication is enabled by default. You can also enable token-based authentication.
11. AKS - To disable authentication, set the auth_enabled=False parameter
12. AKS - If key authentication is enabled, you can use the service.get_keys() method to retrieve a primary and secondary authentication key
13. AKS - If token authentication is enabled, you can use the service.get_token() method to retrieve a JWT token and that token's expiration time
14. auth_enabled Default value is false for ACI and true for AKS.
15. Authentication Method	ACI					AKS
	Key						Disabled by default	Enabled by default
	Token					Not Available		Disabled by default
16. The key or token must be formatted as Bearer <key-or-token>
17. The primary difference between keys and tokens is that keys are static and can be regenerated manually, and tokens need to be refreshed upon expiration.
18. If you need to regenerate a key, use service.regen_key().
19. Currently the only way to retrieve the token is by using the Azure Machine Learning SDK or the Azure CLI machine learning extension.
20. The ModelDataCollector class enables you to define a data collector for your models in Azure Machine Learning AKS deployments. The data collector object can be used to collect model data, such as inputs and predictions, to the blob storage of the workspace. When model data collection is enabled in your deployment, collected data will show up in the following container path as csv files: /modeldata/{workspace_name}/{webservice_name}/{model_name}/{model_version}/{designation}/{year}/{month}/{day}/{collection_name}.csv
21. Currently, ModelDataCollector only works in Azure Machine Learning AKS deployments.


Model as Batch Service -

1. Using the ParallelRunStep class, you can read batches of files from a File dataset and write the processing output to a PipelineData reference.
2. you can set the output_action setting for the step to "append_row", which will ensure that all instances of the step being run in parallel will collate their results to a single output file named parallel_run_step.txt. 
3. ParallelRunStep mini_batch_size config is The number of files the scoring_script can process in one run() call. Default is 10.
4. Compute clusters support ONLY Batch Inference. NO support for realtime inference.
5. compute.instance_count - The number of compute nodes to be used for batch scoring. Default is 1.
6. A batch scoring workload runs as an offline job.
7. If the output file exists, the batch scoring job will fail.
8. az ml job show --name $job_name - Use job show to check details and status of a batch scoring job.
9. There are two top-level log folders, azureml-logs and logs.
	

Accuracy: (TP+TN)/(TP+TN+FP+FN) - in other words, out all of the predictions, how many were correct?
Recall: TP/(TP+FN) - in other words, of all the cases that are positive, how many did the model identify?
Precision: TP/(TP+FP) - in other words, of all the cases that the model predicted to be positive, how many actually are positive?


Hyper parameter tuning -

1. The set of hyperparameter values tried during hyperparameter tuning is known as the search space.
2. Types of Discrete Hyperparameters -
	Choice
	qnormal
	quniform
	qlognormal
	qloguniform
3. Types of Continuous Hyperparameters -
	normal
	uniform
	lognormal
	loguniform
4. Grid sampling can only be employed when all hyperparameters are discrete, and is used to try every possible combination of parameters in the search space.
5. Random sampling is used to randomly select a value for each hyperparameter, which can be a mix of discrete and continuous values
6. Bayesian sampling chooses hyperparameter values based on the Bayesian optimization algorithm, which tries to select parameter combinations that will result in improved performance from the previous selection.
7. You can only use Bayesian sampling with choice, uniform, and quniform parameter expressions, and you can't combine it with an early-termination policy.
8. To help prevent wasting time, you can set an early termination policy that abandons runs that are unlikely to produce a better result than previously completed runs.
9. The policy is evaluated at an evaluation_interval you specify, based on each time the target performance metric is logged. 
10. You can also set a delay_evaluation parameter to avoid evaluating the policy until a minimum number of iterations have been completed.
11. bandit policy to stop a run if the target performance metric underperforms the best run so far by a specified margin.
12. slack_amount / slack_factor - compares best performance metric 
13. median stopping policy abandons runs where the target performance metric is worse than the median of the running averages for all runs
14. truncation selection policy cancels the lowest performing X% of runs at each evaluation interval based on the truncation_percentage value you specify for X.
15. Hyperdrice scoring sript must -
	Include an argument for each hyperparameter you want to vary.
	Log the target performance metric.
	
Differential Privacy -

1. Differential privacy seeks to protect individual data values by adding statistical "noise" to the analysis process. 
2. The amount of variation caused by adding noise is configurable through a parameter called epsilon. 
3. A low epsilon value provides the most privacy, at the expense of less accuracy when aggregating the data. 
4. A higher epsilon value results in aggregations that are more true to the actual data distribution, but in which the individual contribution of a single individual to the aggregated value is less obscured by noise.
5. Use SmartNoise to generate differentially private analysis.
6. Epsilon is correlated with another value named delta, that indicates the probability that a report generated by an analysis is not fully private.
7. The higher the delta, the higher the epsilon. 

Model Explanations -

1. Visualizations are only available for experiment runs that were configured to generate and upload explanations. 
2. When using automated machine learning, only the run producing the best model has explanations generated by default.
3. Visualizing summary importance - You can view the features as a swarm plot, a box plot, or a violin plot.
4. Global feature importance quantifies the relative importance of each feature in the test dataset as a whole.
5. Local feature importance measures the influence of each feature value for a specific individual prediction.
6. For a multi-class classification model, a local importance values for each possible class is calculated for every feature, with the total across all classes always being 0.
7. For a regression model, there are no classes so the local importance values simply indicate the level of influence each feature has on the predicted scalar label.
8. MimicExplainer - An explainer that creates a global surrogate model that approximates your trained model and can be used to generate explanations. This explainable model must have the same kind of architecture as your trained model (for example, linear or tree-based).
9. TabularExplainer - An explainer that acts as a wrapper around various SHAP explainer algorithms, automatically choosing the one that is most appropriate for your model architecture.
10. PFIExplainer - a Permutation Feature Importance explainer that analyzes feature importance by shuffling feature values and measuring the impact on prediction performance.
11. Global Explainer code is the same for MimicExplainer and TabularExplainer. The PFIExplainer requires the actual labels that correspond to the test features.
12. Local Explainer code is the same for MimicExplainer and TabularExplainer. The PFIExplainer doesn't support local feature importance explanations.
13. The InterpretML package supports a wide variety of interpretability techniques such as SHapley Additive exPlanations (SHAP), mimic explainer and permutation feature importance (PFI).
14. The explanation functions accept both models and pipelines as input. 

Model Fairness -

1. Mitigation algorithms -
    Exponentiated Gradient -A reduction technique that applies a cost-minimization approach to learning the optimal trade-off of overall predictive performance and fairness disparity
    Grid Search - A simplified version of the Exponentiated Gradient algorithm that works efficiently with small numbers of constraints
    Threshold Optimizer - A post-processing technique that applies a constraint to an existing classifier, transforming the prediction as appropriate
2. Demographic parity:this constraint tries to ensure that an equal number of positive predictions are made in each group.
3. True positive rate parity:  this constraint tries to ensure that each group contains a comparable ratio of true positive predictions.
4. False-positive rate parity: this constraint tries to ensure that each group contains a comparable ratio of false-positive predictions.
5. Equalized odds:this constraint tries to ensure that each group contains a comparable ratio of true positive and false-positive predictions.
6. Error rate parity: ensure that the error for each sensitive feature group does not deviate from the overall error rate by more than a specified amount.
7. Bounded group loss: this constraint with any of the reduction-based mitigation algorithms to restrict the loss for each sensitive feature group in a regression model.
8. reduction-based mitigation algorithms - Exponentiated Gradient and Grid Search
9. An important point to reinforce is that applying fairness mitigation to a model is a trade-off between overall predictive performance and disparity across sensitive feature groups
10. Disparity metrics can evaluate and compare model's behavior across different groups either as ratios or as differences. 

Data Drift -

1. This change in data profiles between training and inferencing is known as data drift
2. You can define a schedule to run every Day, Week, or Month.
3. you can specify a latency, indicating the number of hours to allow for new data to be collected and added to the target dataset
4. Data drift is measured using a calculated magnitude of change in the statistical distribution of feature values over time.

RBAC -

1. ML Studio workspace access roles -
	Owner - Grants full access to manage all resources, including the ability to assign roles in Azure RBAC.
	Contributor - Grants full access to manage all resources, but does not allow you to assign roles in Azure RBAC, manage assignments in Azure Blueprints, or share image galleries.
	Reader - View all resources, but does not allow you to make any changes.
2. Ways to assign roles -
	Azure portal UI
	PowerShell
	Azure CLI
	REST API
	Azure Resource Manager templates
3. az ml workspace share -w my_workspace -g my_resource_group --role Contributor --user jdoe@contoson.com
4. {
    "Name": "Data Scientist Custom",
    "IsCustom": true,
    "Description": "Can run experiment but can't create or delete compute.",
    "Actions": ["*"],
    "NotActions": [
        "Microsoft.MachineLearningServices/workspaces/*/delete",
        "Microsoft.MachineLearningServices/workspaces/write",
        "Microsoft.MachineLearningServices/workspaces/computes/*/write",
        "Microsoft.MachineLearningServices/workspaces/computes/*/delete", 
        "Microsoft.Authorization/*/write"
    ],
    "AssignableScopes": [
        "/subscriptions/<subscription_id>/resourceGroups/<resource_group_name>/providers/Microsoft.MachineLearningServices/workspaces/<workspace_name>"
    ]
	}
	
5. You can change the AssignableScopes field to set the scope of this custom role at the subscription level, the resource group level, or a specific workspace level.
6. az role definition create --role-definition data_scientist_role.json
7. az role definition update --role-definition update_def.json --subscription <sub-id>
8. If you receive a failure when trying to create a workspace for the first time, make sure that your role allows Microsoft.MachineLearningServices/register/action.
9. Managed identity is only supported when using the Azure Machine Learning SDK from an Azure Virtual Machine or with an Azure Machine Learning compute cluster.
10. from azureml.core.authentication import InteractiveLoginAuthentication
	interactive_auth = InteractiveLoginAuthentication(tenant_id="your-tenant-id")
11. If you are using the SDK from an environment where you have previously authenticated interactively using the Azure CLI, you can use the AzureCliAuthentication class to authenticate to the workspace using the credentials cached by the CLI
	from azureml.core.authentication import AzureCliAuthentication
	cli_auth = AzureCliAuthentication()

ML Designer -

1.Due to datstore access limitation, if your inference pipeline contains Import Data module, it will be auto-removed when deploy to real-time endpoint.
2. If you have any doubt about whether missing values were changed, select the option, Generate missing value indicator column. 
3. SMOTE takes the entire dataset as an input, but it increases the percentage of only the minority cases.
4. To increase the percentage of minority cases to twice the previous percentage, you would enter 200 for SMOTE percentage in the module's properties.
5. SMOTE is intended for improving a model during training, not for scoring.
6. Filter based Feature selection chi squared - Use this method for computing feature importance for two categorical columns.
7. If you are going to use Filter Based Feature Selection in inference, you need to use Select Columns Transform to store the feature selected result and Apply Transformation to apply the feature selected transformation to the scoring dataset
8. The designer lets you save data transformations as datasets so that you can use them in other pipelines.
9. Designer Cross validation algorithm defaults to 10 folds
10. It's a best practice to normalize datasets before you use them for cross-validation.
11. There's no need to split the dataset into training and testing sets when you use cross-validation.
12. The Web Service Input module and Output module can only connect to the type DataFrameDirectory. 
13. The Web Service Input module indicates where user data enters the pipeline. The Web Service Output module indicates where user data is returned in a real-time inference pipeline.
14. Enter Data Manually provides the data schema for web service input and is necessary for deploying the real-time endpoint. 
15. Clean Missing Data - The left port contains the the cleaned data. The right port contains the discarded data.
16. Train Model - The left port contains the the training set. The right port contains the test set.
17. When you select Create inference pipeline, several things happen:
		The trained model is stored as a Dataset module in the module palette. You can find it under My Datasets.
		Training modules like Train Model and Split Data are removed.
		The saved trained model is added back into the pipeline.
		Web Service Input and Web Service Output modules are added. These modules show where user data enters the pipeline and where data is returned.


AutoML -

1. AutoML supports only Tabular dataset.
2. Forecasting only supports k-fold cross validation.
3. Exit criterion - Training job time (hours), Metric score threshold
4. If the dataset is less than 1,000 rows, 10 folds are used.
5. If the rows are between 1,000 and 20,000, then three folds are used.
6. "featurization": 'FeaturizationConfig'	Indicates customized featurization step should be used. 
7. StandardScalerWrapper technique is used to scale and normalize the data prior to training
8. Automatic Featurization - 
	Drop high cardinality or no variance features
	Impute missing values
	Generate more features
	Transform and encode
	Word embeddings
	Cluster Distance
9. Data Guardrails -
	Missing feature values imputation
	High cardinality feature handling
	Validation split handling
	Class balancing detection
	Memory issues detection
	Frequency detection
10. Block transformers - Specifies block transformers to be used in the featurization process.
11. BERT is used in the featurization layer of AutoML. In this layer, if a column contains free text or other types of data like timestamps or simple numbers, then featurization is applied accordingly.
12. Types of scalers -
	StandardScaleWrapper - Standardize features by removing the mean and scaling to unit variance
	MinMaxScalar - Transforms features by scaling each feature by that column's minimum and maximum
	MaxAbsScaler - Scale each feature by its maximum absolute value
	RobustScalar - Scales features by their quantile range
13. The Caruana ensemble selection algorithm with sorted ensemble initialization is used to decide which models to use within the ensemble. At a high level, this algorithm initializes the ensemble with up to five models with the best individual scores, and verifies that these models are within 5% threshold of the best score to avoid a poor initial ensemble. 
14. The web interface for automated ML always uses a remote compute target. 
15. The Many Models Solution Accelerator (preview) builds on Azure Machine Learning and enables you to use automated ML to train, operate, and manage hundreds or even thousands of machine learning models.
16. Azure Databricks supported with Python SDK AutoML only.
17. Once the models are in the ONNX format, they can be run on a variety of platforms and devices. 
18. For AutoML register model API, modelid is model_name and it doesn't need model_path
19. Python 3.8 is not compatible with automl.
20. ONNX only supports classification and regression tasks at this time.
21. max_concurrent_iterations defaults to 1.


Data Labeling -

1. Data labeling in Azure Machine Learning is in public preview.
2. Data Labeling project types -
	Image classification Multi-class
	Image classification Multi-label
	Object Identification (Bounding Box)
	Instance Segmentation (Polygon)
3. COCO is a large-scale object detection, segmentation, and captioning dataset.
4. Steps to perform -
	Create a datastore
	Create a labeling project
	Select or create a dataset
	Incremental refresh
	Label classes
	Labeling instructions
	ML assisted labeling
	Tag the images
	Review labeled data
	Export labeled data


Discoveries -

1. ML Designer supports only Tabular Datasets
2. ML Designer implements SQLLite. It does not have RIGHT OUTER join. Also its views are read only.
3. ML Designer Import Data gets the data diectly without registering it.
4. PipelineData object stays at Pipeline level, above steps level.
5. There is no module called Load Dataset in ML Designer.
6. There is no run.get_log() API available.
7. CosmosDB is not a valid datastore source.
8. Service principal authentication is valid for ADLS. 
9. ScikitLearn, PyTorch, Tensorflow etc are NOT part of base environment configurations.
10. In AutoML FeaturizationConfig, there is no enable_transformer() method.
11. Default Authentication method - AKS - Key, Token has to be enable explicitly.
12. AKS default autoscaling target utilization is 70%, service timeout is 1s.
13. AutoML cross validation 10% by default, to change it - either give validation data or validation size
14. ACI can be authenticated with key ONLY.
15. In AutoML, train/validation split automatically happens ONLY IF data is greater than 20000 rows.
16. IN AutoML UI, for smaller dataset, cross validation with default 5 fold will happen automatically. 
17. PipelineParameter is at Pipeline level, above step level.
18. You should export data labels in COCO format.
19. az ml workspace sync-keys to refresh keys in workspace.
20. Service pricipal has JWT which is used to get token for ACI authentication.
21. For an experiment, files written to logs folder are uploaded real time.
22. Compute cluster is the only type of ML Studio which can be associated to Designer.
23. Databricks, HDInsight cannot be connected to Designer.
24. Spearman Correlation is a Primary metric for AutoML Regression or Timeseries Forecasting task.
25. AutoML can work with Databricks cluster.
26. AutoML Regression Task does NOT support deep learning.
27. Open Neural Network Exchange (ONNX) - Models from many frameworks including TensorFlow, PyTorch, SciKit-Learn, Keras, Chainer, MXNet, MATLAB, and SparkML can be exported or converted to the standard ONNX format.
28. Homomorphic encryption allows for computations to be done on encrypted data without requiring access to a secret (decryption) key.
29. Microsoft SEAL is an open-source homomorphic encryption library that allows additions and multiplications to be performed on encrypted integers or real numbers. 


Reference Links -
https://docs.microsoft.com/en-us/azure/machine-learning/algorithm-module-reference/module-reference
https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-automated-ml-for-ml-models
https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-labeling
https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-pytorch
https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-parallel-run-step#monitor-the-parallel-run-job
https://docs.microsoft.com/en-us/azure/machine-learning/how-to-assign-roles
https://docs.microsoft.com/id-id/azure/machine-learning/how-to-deploy-and-where
https://docs.microsoft.com/en-us/azure/machine-learning/concept-train-model-git-integration
https://docs.microsoft.com/en-us/azure/machine-learning/concept-differential-privacy
https://docs.microsoft.com/en-us/azure/databricks/



***Always check examp prep site legitimacy on -
http://certguard.com/Search.asp

These are BrainDump sites. Stay away ...
https://www.certlibrary.com/info/DP-100
https://www.itexams.com/info/DP-100
https://www.examtopics.com/exams/microsoft/dp-100/

safe site -
www.whizlabs.com
www.measureup.com
