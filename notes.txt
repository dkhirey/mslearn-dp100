Model Explanations -

1. Visualizations are only available for experiment runs that were configured to generate and upload explanations. 
2. When using automated machine learning, only the run producing the best model has explanations generated by default.
3. Visualizing summary importance - You can view the features as a swarm plot, a box plot, or a violin plot.
4. Global feature importance quantifies the relative importance of each feature in the test dataset as a whole.
5. Local feature importance measures the influence of each feature value for a specific individual prediction.
6. For a multi-class classification model, a local importance values for each possible class is calculated for every feature, with the total across all classes always being 0.
7. For a regression model, there are no classes so the local importance values simply indicate the level of influence each feature has on the predicted scalar label.
8. MimicExplainer - An explainer that creates a global surrogate model that approximates your trained model and can be used to generate explanations. This explainable model must have the same kind of architecture as your trained model (for example, linear or tree-based).
9. TabularExplainer - An explainer that acts as a wrapper around various SHAP explainer algorithms, automatically choosing the one that is most appropriate for your model architecture.
10. PFIExplainer - a Permutation Feature Importance explainer that analyzes feature importance by shuffling feature values and measuring the impact on prediction performance.
11. Global Explainer code is the same for MimicExplainer and TabularExplainer. The PFIExplainer requires the actual labels that correspond to the test features.
12. Local Explainer code is the same for MimicExplainer and TabularExplainer. The PFIExplainer doesn't support local feature importance explanations.
